# Web-Crawler

Данный проект представляет собой CLI утилиту для рекурсивного обхода страниц Википедии и сбора уникальных ссылок на другие статьи. Программа сохраняет найденные ссылки в базе данных SQLite, и продолжает обрабатывать страницы до достижения глубины 6.

Для ускорения обработки ссылок и параллельной обработки нескольких страниц одновременно, программа использует многопоточность, которая реализована с использованием стандартной библиотеки threading. Также этот код покрыт тестами, которые проверяют его работоспособность.

## Установка и использование
1. Для развертывания проекта клонируйте репозиторий:
```bash
git clone https://github.com/yourusername/wiki-link-crawler.git
cd wiki-link-crawler
```
2. Убедитесь, что установлен Python версии 3.10+:
```bash
python --version
```
3. Для запуска основного скрипта для обхода страниц используйте следующую команду:
```bash
python wiki_crawler.py "<URL>"
```
Вместо <URL> вставить ссылку на Википедию, например python wiki_crawler.py "https://en.wikipedia.org/wiki/Web_crawler"

4. Для запуска тестов используйте следующую команду:
```bash
python -m unittest test_wiki_crawler.py
```
